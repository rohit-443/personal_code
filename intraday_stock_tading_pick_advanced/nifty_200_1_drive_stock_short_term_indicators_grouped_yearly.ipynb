{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyp1JOVRHuYw"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "EVvBUebWRBs9",
        "outputId": "bf304673-c429-41a0-cffe-53c5409a29ee"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"tickerData\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88b1d2b6"
      },
      "source": [
        "## Access gmail credentials\n",
        "\n",
        "### Subtask:\n",
        "Retry securely retrieving the Gmail address and app password from Colab secrets after the previous attempt failed due to a `SecretNotFoundError`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_062Yc7Lt4EJ",
        "outputId": "e305147c-458f-4711-fcef-f7fb83b3067b"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "gmail_address = userdata.get('GMAIL_ADDRESS')\n",
        "gmail_app_password = userdata.get('GMAIL_APP_PASSWORD')\n",
        "\n",
        "print(\"Gmail address retrieved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQiqRWgqFpDP",
        "outputId": "fa802335-8648-4f82-c285-a35a49f5a9f6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb8hebBrkRM7"
      },
      "outputs": [],
      "source": [
        "ticker_price_history_df= spark.read.csv(\"/content/drive/MyDrive/ticker_prices_nif_100_1/1_year_history.csv\", header=True, inferSchema=True)\n",
        "ticker_price_history_df.write.saveAsTable(\"ticker_price_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD3oTWLJH3Jt",
        "outputId": "682a7301-83aa-49bd-ef3e-162f32575027"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import coint\n",
        "tickers = sorted(list(set([\n",
        "    \"SWIGGY.NS\", \"ABB.NS\", \"ACC.NS\", \"AUBANK.NS\", \"ADANIENSOL.NS\", \"ADANIENT.NS\", \"ADANIGREEN.NS\", \"ADANIPOWER.NS\", \"ATGL.NS\", \"ABCAPITAL.NS\", \"ALKEM.NS\", \"AMBUJACEM.NS\", \"APOLLOHOSP.NS\", \"ASHOKLEY.NS\", \"ASIANPAINT.NS\", \"ASTRAL.NS\", \"AUROPHARMA.NS\", \"DMART.NS\", \"AXISBANK.NS\", \"BSE.NS\", \"BAJAJ-AUTO.NS\", \"BAJFINANCE.NS\", \"BAJAJFINSV.NS\", \"BAJAJHLDNG.NS\", \"BANKBARODA.NS\", \"BANKINDIA.NS\", \"BDL.NS\", \"BEL.NS\", \"BHARATFORG.NS\", \"BHEL.NS\", \"BPCL.NS\", \"BHARTIARTL.NS\", \"BHARTIHEXA.NS\", \"BIOCON.NS\", \"BLUESTARCO.NS\", \"BOSCHLTD.NS\", \"BRITANNIA.NS\", \"CGPOWER.NS\", \"CANBK.NS\", \"CHOLAFIN.NS\", \"COALINDIA.NS\", \"COCHINSHIP.NS\", \"COLPAL.NS\", \"CONCOR.NS\", \"COROMANDEL.NS\", \"CUMMINSIND.NS\", \"DLF.NS\", \"DABUR.NS\", \"DIVISLAB.NS\", \"DIXON.NS\", \"EICHERMOT.NS\", \"EXIDEIND.NS\", \"NYKAA.NS\", \"FEDERALBNK.NS\", \"FORTIS.NS\", \"GAIL.NS\", \"GMRAIRPORT.NS\", \"GLENMARK.NS\", \"GODFRYPHLP.NS\", \"GODREJCP.NS\", \"GODREJPROP.NS\", \"GRASIM.NS\", \"HCLTECH.NS\", \"HDFCAMC.NS\", \"HDFCBANK.NS\", \"HDFCLIFE.NS\", \"HAVELLS.NS\", \"HEROMOTOCO.NS\", \"HINDALCO.NS\", \"HAL.NS\", \"HINDPETRO.NS\", \"HINDUNILVR.NS\", \"POWERINDIA.NS\", \"HUDCO.NS\", \"HYUNDAI.NS\", \"ICICIBANK.NS\", \"ICICIGI.NS\", \"IRB.NS\", \"ITCHOTELS.NS\", \"INDIANB.NS\", \"IOC.NS\", \"IRCTC.NS\", \"IRFC.NS\", \"IREDA.NS\", \"IGL.NS\", \"INDUSTOWER.NS\", \"INDUSINDBK.NS\", \"NAUKRI.NS\", \"INDIGO.NS\", \"JSWENERGY.NS\"\n",
        "])))\n",
        "for i in tickers:\n",
        "  data = yf.download(i, period=\"5d\", interval=\"1d\")\n",
        "  data.to_csv(f\"{i}.csv\")\n",
        "  data_df= spark.read.csv(f\"{i}.csv\", header=True, inferSchema=True)\n",
        "  data_df= data_df.withColumn(\"Ticker\", lit(f\"{i}\"))\n",
        "  # Filter out rows where 'Price' column contains \"Ticker\" or \"Date\"\n",
        "  data_df_filtered = data_df.filter((col(\"Price\") != lit(\"Ticker\")) & (col(\"Price\") != lit(\"Date\")))\\\n",
        "  .withColumnRenamed(\"Price\", \"Date\")\n",
        "  # Write the filtered data_df to the CSV\n",
        "  data_df_filtered.write.option(\"header\",\"true\")\\\n",
        "  .mode(\"append\")\\\n",
        "  .csv(\"5_days_history.csv\")\n",
        "\n",
        "  # The commented out code below seems to be an alternative approach that is not needed with the current fix.\n",
        "  # data_df = spark.createDataFrame(data)\n",
        "  # data_df_renamed = data_df\n",
        "  # for j in data_df.columns:\n",
        "  #   data_df_renamed = data_df_renamed.withColumnRenamed(j, j.split(\",\")[0][2:-1])\n",
        "  # data_df_ticker = data_df_renamed.withColumn(\"Ticker\", lit(f\"{i}\"))\n",
        "  # data_df_ticker.write.option(\"header\",\"true\")\\\n",
        "  # .mode(\"append\")\\\n",
        "  # .csv(\"1_year_history.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXycy5rHl0G9"
      },
      "outputs": [],
      "source": [
        "ticker_5day_df = spark.read.csv(\"5_days_history.csv\", header= True, inferSchema= True)\n",
        "ticker_5day_df.write.saveAsTable(\"ticker_5day_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e61edfa4",
        "outputId": "473d8aeb-d630-44de-c81e-f93c71009a0c"
      },
      "outputs": [],
      "source": [
        "# Read the tables into DataFrames\n",
        "target_df = spark.read.table(\"ticker_price_history\")\n",
        "source_df = spark.read.table(\"ticker_5day_history\")\n",
        "\n",
        "# Perform a full outer join on Date and Ticker\n",
        "merged_df = target_df.join(source_df, [\"Date\", \"Ticker\"], \"fullouter\")\n",
        "\n",
        "# Use coalesce to select the most recent data from source if available, otherwise use target data\n",
        "merged_df = merged_df.select(\n",
        "    coalesce(source_df[\"Date\"], target_df[\"Date\"]).alias(\"Date\"),\n",
        "    coalesce(source_df[\"Ticker\"], target_df[\"Ticker\"]).alias(\"Ticker\"),\n",
        "    coalesce(source_df[\"Close\"], target_df[\"Close\"]).alias(\"Close\"),\n",
        "    coalesce(source_df[\"High\"], target_df[\"High\"]).alias(\"High\"),\n",
        "    coalesce(source_df[\"Low\"], target_df[\"Low\"]).alias(\"Low\"),\n",
        "    coalesce(source_df[\"Open\"], target_df[\"Open\"]).alias(\"Open\"),\n",
        "    coalesce(source_df[\"Volume\"], target_df[\"Volume\"]).alias(\"Volume\")\n",
        ")\n",
        "\n",
        "# To \"merge\" the data, you would typically overwrite the original table or save to a new location.\n",
        "# Overwriting the original table:\n",
        "# merged_df.write.mode(\"overwrite\").saveAsTable(\"ticker_price_history\")\n",
        "\n",
        "# Or save to a new location:\n",
        "merged_df.write.option(\"header\",\"true\").mode(\"overwrite\").csv(\"/content/drive/MyDrive/ticker_prices_nif_100_1/1_year_history.csv\")\n",
        "merged_df.write.option(\"header\",\"true\").mode(\"overwrite\").csv(\"1_year_history.csv\")#optional\n",
        "\n",
        "merged_df.agg(min(\"Date\")).show()\n",
        "merged_df.agg(max(\"Date\")).show()\n",
        "# For now, let's show the head of the merged DataFrame to verify\n",
        "# display(merged_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT6awP8-QYNQ"
      },
      "outputs": [],
      "source": [
        "ticker_final_df = spark.read.csv(\"/content/drive/MyDrive/ticker_prices_nif_100_1/1_year_history.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBa0Gss0RsBj"
      },
      "outputs": [],
      "source": [
        "# ticker_final_df.write.option(\"header\",\"true\")\\\n",
        "#   .mode(\"overwrite\")\\\n",
        "#   .csv(\"/content/drive/MyDrive/ticker_prices/1_year_history.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuB3D5FORzMr"
      },
      "outputs": [],
      "source": [
        "# ticker_final_df_1= spark.read.csv(\"/content/drive/MyDrive/ticker_prices/1_year_history.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZPcHLkHvd-O",
        "outputId": "caabc611-fe06-4be1-c68d-e3d01898c155"
      },
      "outputs": [],
      "source": [
        "ticker_final_df.select(\"Ticker\").distinct().show()\n",
        "ticker_final_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4OBW8CWxsGgV",
        "outputId": "cb977e0c-6660-4def-f3b9-f71278029b77"
      },
      "outputs": [],
      "source": [
        "one_yr_df = ticker_final_df.toPandas()\n",
        "one_yr_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qot7FXLrFZOr",
        "outputId": "33b20502-0a04-49c3-a3e0-aff35af82ab7"
      },
      "outputs": [],
      "source": [
        "# tickers = [\"INFY.NS\", \"TCS.NS\", \"HDFCBANK.NS\", \"ICICIBANK.NS\"]\n",
        "# Initialize an empty DataFrame to store the combined results\n",
        "combined_tick_df = pd.DataFrame()\n",
        "\n",
        "for i in tickers:\n",
        "  tick_one_yr_df = ticker_final_df.select(\"*\").filter(col(\"Ticker\") == lit(f\"{i}\")).orderBy(\"Date\").toPandas()\n",
        "\n",
        "  # Convert relevant columns to numeric, coercing errors\n",
        "  numeric_cols = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
        "  for col_name in numeric_cols:\n",
        "      tick_one_yr_df[col_name] = pd.to_numeric(tick_one_yr_df[col_name], errors='coerce')\n",
        "\n",
        "  #calculating SMA\n",
        "  tick_one_yr_df['SMA20'] = tick_one_yr_df['Close'].rolling(window=20).mean()\n",
        "  tick_one_yr_df['SMA50'] = tick_one_yr_df['Close'].rolling(window=50).mean()\n",
        "  tick_one_yr_df['SMA200'] = tick_one_yr_df['Close'].rolling(window=200).mean()\n",
        "\n",
        "  #calculating MACD\n",
        "  ema12 = tick_one_yr_df['Close'].ewm(span=12, adjust=False).mean()\n",
        "  ema26 = tick_one_yr_df['Close'].ewm(span=26, adjust=False).mean()\n",
        "  tick_one_yr_df['MACD'] = ema12 - ema26\n",
        "  tick_one_yr_df['MACD_Signal'] = tick_one_yr_df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "  #calculating RSI\n",
        "  delta = tick_one_yr_df['Close'].diff()\n",
        "  gain = delta.clip(lower=0)\n",
        "  loss = -delta.clip(upper=0)\n",
        "  # Wilder's smoothing (EMA-like)\n",
        "  avg_gain = gain.ewm(alpha=1/14, min_periods=14, adjust=False).mean()\n",
        "  avg_loss = loss.ewm(alpha=1/14, min_periods=14, adjust=False).mean()\n",
        "  rs = avg_gain / avg_loss\n",
        "  tick_one_yr_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "\n",
        "\n",
        "  #calculating william %R\n",
        "  high14 = tick_one_yr_df['High'].rolling(window=14).max()\n",
        "  low14 = tick_one_yr_df['Low'].rolling(window=14).min()\n",
        "  tick_one_yr_df['Williams_%R'] = -100 * (high14 - tick_one_yr_df['Close']) / (high14 - low14)\n",
        "\n",
        "  #calculating MFI\n",
        "  tp = (tick_one_yr_df['High'] + tick_one_yr_df['Low'] + tick_one_yr_df['Close']) / 3\n",
        "  mf = tp * tick_one_yr_df['Volume']\n",
        "  pos_mf = mf.where(tp > tp.shift(1), 0)\n",
        "  neg_mf = mf.where(tp < tp.shift(1), 0)\n",
        "  rolling_pos = pos_mf.rolling(window=14).sum()\n",
        "  rolling_neg = neg_mf.rolling(window=14).sum()\n",
        "  # Avoid division by zero\n",
        "  mfr = rolling_pos / (rolling_neg.replace(0, np.nan))\n",
        "  tick_one_yr_df['MFI'] = 100 - (100 / (1 + mfr))\n",
        "\n",
        "  #calculating cci\n",
        "  tp = (tick_one_yr_df['High'] + tick_one_yr_df['Low'] + tick_one_yr_df['Close']) / 3\n",
        "  sma_tp = tp.rolling(window=20).mean()\n",
        "  mad = tp.rolling(window=20).apply(lambda x: (x - x.mean()).abs().mean(), raw=False) # Removed raw=True\n",
        "  tick_one_yr_df['cci'] = (tp - sma_tp) / (0.015 * mad)\n",
        "\n",
        "  # Calculating On-Balance Volume (OBV)\n",
        "  # Initialize OBV with 0\n",
        "  tick_one_yr_df['OBV'] = 0\n",
        "  # Calculate OBV based on price changes using .loc for assignment\n",
        "  for j in range(1, len(tick_one_yr_df)):\n",
        "      if tick_one_yr_df['Close'].iloc[j] > tick_one_yr_df['Close'].iloc[j-1]:\n",
        "          tick_one_yr_df.loc[j, 'OBV'] = tick_one_yr_df['OBV'].iloc[j-1] + tick_one_yr_df['Volume'].iloc[j]\n",
        "      elif tick_one_yr_df['Close'].iloc[j] < tick_one_yr_df['Close'].iloc[j-1]:\n",
        "          tick_one_yr_df.loc[j, 'OBV'] = tick_one_yr_df['OBV'].iloc[j-1] - tick_one_yr_df['Volume'].iloc[j]\n",
        "      else:\n",
        "          tick_one_yr_df.loc[j, 'OBV'] = tick_one_yr_df['OBV'].iloc[j-1]\n",
        "\n",
        "  # Calculating Accumulation/Distribution Line (A/D Line)\n",
        "  # Money Flow Multiplier (MFM)\n",
        "  # Ensure High and Low are not equal to avoid division by zero\n",
        "  mfm_denominator = tick_one_yr_df['High'] - tick_one_yr_df['Low']\n",
        "  mfm = ((tick_one_yr_df['Close'] - tick_one_yr_df['Low']) - (tick_one_yr_df['High'] - tick_one_yr_df['Close'])) / mfm_denominator\n",
        "  mfm = mfm.replace([np.inf, -np.inf], np.nan).fillna(0) # Handle potential division by zero and NaNs\n",
        "  # Money Flow Volume (MFV)\n",
        "  mfv = mfm * tick_one_yr_df['Volume']\n",
        "  # Accumulation/Distribution Line (A/D Line) is the cumulative sum of MFV\n",
        "  tick_one_yr_df['AD_Line'] = mfv.cumsum()\n",
        "\n",
        "\n",
        "  # Append the processed DataFrame for the current ticker to the combined DataFrame\n",
        "  combined_tick_df = pd.concat([combined_tick_df, tick_one_yr_df], ignore_index=True)\n",
        "\n",
        "  # For min and max values of a column in pandas DataFrame\n",
        "  print(combined_tick_df[\"Date\"].min())\n",
        "  print(combined_tick_df[\"Date\"].max())\n",
        "\n",
        "\n",
        "\n",
        "# Now combined_tick_df contains the data for all tickers with SMA values and new indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c066e7fa"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.dates as mdates\n",
        "\n",
        "# # Convert 'Date' column to datetime objects if not already\n",
        "# combined_tick_df['Date'] = pd.to_datetime(combined_tick_df['Date'])\n",
        "\n",
        "# # Get the list of unique tickers\n",
        "# unique_tickers = combined_tick_df['Ticker'].unique()\n",
        "\n",
        "# # Plot for each ticker\n",
        "# for ticker in unique_tickers:\n",
        "#     ticker_df = combined_tick_df[combined_tick_df['Ticker'] == ticker].copy()\n",
        "\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.plot(ticker_df['Date'], ticker_df['Close'] , label='Close Price')\n",
        "#     plt.plot(ticker_df['Date'], ticker_df['SMA20'] , label='SMA 20')\n",
        "#     plt.plot(ticker_df['Date'], ticker_df['SMA50'] , label='SMA 50')\n",
        "#     plt.plot(ticker_df['Date'], ticker_df['SMA200'] , label='SMA 200')\n",
        "\n",
        "#     plt.xlabel('Date')\n",
        "#     plt.ylabel('Price')\n",
        "#     plt.title(f'{ticker} Stock Price and SMAs over Time')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     # Format x-axis to show months\n",
        "#     ax = plt.gca()\n",
        "#     formatter = mdates.DateFormatter('%Y-%m')\n",
        "#     ax.xaxis.set_major_formatter(formatter)\n",
        "#     plt.xticks(rotation=45)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dce33452",
        "outputId": "cb88023c-8695-4ee6-abd5-0a31cb2c9dbb"
      },
      "outputs": [],
      "source": [
        "filtered_tickers_df = pd.DataFrame()\n",
        "\n",
        "# Convert 'Date' column to datetime objects if not already\n",
        "combined_tick_df['Date'] = pd.to_datetime(combined_tick_df['Date'])\n",
        "\n",
        "# Get the list of unique tickers\n",
        "unique_tickers = combined_tick_df['Ticker'].unique()\n",
        "\n",
        "for ticker in combined_tick_df['Ticker'].unique():\n",
        "    ticker_df = combined_tick_df[combined_tick_df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # Calculate previous day's values\n",
        "    ticker_df['RSI_prev'] = ticker_df['RSI'].shift(1)\n",
        "    ticker_df['MACD_prev'] = ticker_df['MACD'].shift(1)\n",
        "    ticker_df['Williams_%R_prev'] = ticker_df['Williams_%R'].shift(1)\n",
        "    ticker_df['cci_prev'] = ticker_df['cci'].shift(1)\n",
        "\n",
        "\n",
        "    # Define the filtering conditions based on the latest proposal\n",
        "    condition_rsi = (ticker_df['RSI'] >= 55) & (ticker_df['RSI'] > ticker_df['RSI_prev'])\n",
        "    condition_williams = (ticker_df['Williams_%R'] >= -22) & (ticker_df['Williams_%R'] > ticker_df['Williams_%R_prev'])\n",
        "    condition_macd = ((ticker_df['MACD'] - ticker_df['MACD_Signal']) > 0) & (ticker_df['MACD'] > ticker_df['MACD_prev'])\n",
        "    # Keep CCI condition for potential future use or as an optional filter\n",
        "    condition_cci = (ticker_df['cci'] > -200) & (ticker_df['cci'] < 200)\n",
        "\n",
        "    # Combine all conditions\n",
        "    all_conditions = condition_rsi & condition_macd & condition_williams & condition_cci # Added CCI to combined conditions\n",
        "\n",
        "\n",
        "    # Filter the DataFrame based on all conditions\n",
        "    filtered_df = ticker_df[all_conditions]\n",
        "\n",
        "    # Append the filtered data for the current ticker to the result DataFrame\n",
        "    filtered_tickers_df = pd.concat([filtered_tickers_df, filtered_df], ignore_index=True)\n",
        "\n",
        "# Display the tickers and dates that meet the criteria\n",
        "display(filtered_tickers_df[['Date', 'Ticker', 'RSI', 'MACD', 'MACD_Signal', 'Williams_%R', 'cci']]) # Added CCI to display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0e47eb98",
        "outputId": "01cbd975-66ed-456c-b51d-de4f3d938549"
      },
      "outputs": [],
      "source": [
        "latest_two_days_df = pd.DataFrame()\n",
        "\n",
        "for ticker in combined_tick_df['Ticker'].unique():\n",
        "    ticker_df = combined_tick_df[combined_tick_df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # Sort by date to ensure latest dates are at the end\n",
        "    ticker_df = ticker_df.sort_values(by='Date', ascending=True)\n",
        "\n",
        "    # Get the latest two dates\n",
        "    latest_dates = ticker_df['Date'].tail(2).tolist()\n",
        "\n",
        "    # Filter for the latest two dates\n",
        "    latest_two_days_ticker_df = ticker_df[ticker_df['Date'].isin(latest_dates)]\n",
        "\n",
        "    # Append to the combined DataFrame\n",
        "    latest_two_days_df = pd.concat([latest_two_days_df, latest_two_days_ticker_df], ignore_index=True)\n",
        "\n",
        "# Now apply the filtering logic and categorization to the latest two days data\n",
        "filtered_latest_two_days_df = pd.DataFrame()\n",
        "\n",
        "for ticker in latest_two_days_df['Ticker'].unique():\n",
        "    ticker_df = latest_two_days_df[latest_two_days_df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # Calculate previous day's values (within the latest two days)\n",
        "    ticker_df['RSI_prev'] = ticker_df['RSI'].shift(1)\n",
        "    ticker_df['MACD_prev'] = ticker_df['MACD'].shift(1)\n",
        "    ticker_df['Williams_%R_prev'] = ticker_df['Williams_%R'].shift(1)\n",
        "    # Assuming CCI should also be included based on previous discussions\n",
        "    ticker_df['cci_prev'] = ticker_df['cci'].shift(1)\n",
        "\n",
        "\n",
        "    # Define the filtering conditions based on the latest proposal\n",
        "    condition_rsi = (ticker_df['RSI'] >= 55) & (ticker_df['RSI'] > ticker_df['RSI_prev'])\n",
        "    condition_williams = (ticker_df['Williams_%R'] >= -22) & (ticker_df['Williams_%R'] > ticker_df['Williams_%R_prev'])\n",
        "    condition_macd = ((ticker_df['MACD'] - ticker_df['MACD_Signal']) > 0) & (ticker_df['MACD'] > ticker_df['MACD_prev'])\n",
        "    # Keep CCI condition for potential future use or as an optional filter\n",
        "    condition_cci = (ticker_df['cci'] > -200) & (ticker_df['cci'] < 200) # Example CCI condition within thresholds\n",
        "\n",
        "\n",
        "    # Count how many of the *three proposed* conditions are satisfied for each row\n",
        "    ticker_df['satisfied_conditions_count'] = condition_rsi.astype(int) + \\\n",
        "                                               condition_williams.astype(int) + \\\n",
        "                                               condition_macd.astype(int)\n",
        "\n",
        "\n",
        "    # Categorize based on the count (assuming the user still wants these categories based on the *new* count)\n",
        "    ticker_df['Recommendation'] = 'None'\n",
        "    # Diamond Pick: All 3 proposed conditions satisfied\n",
        "    ticker_df.loc[ticker_df['satisfied_conditions_count'] == 3, 'Recommendation'] = 'Diamond Pick'\n",
        "    # Golden Pick: Any 2 of the 3 proposed conditions satisfied\n",
        "    ticker_df.loc[ticker_df['satisfied_conditions_count'] == 2, 'Recommendation'] = 'Golden Pick'\n",
        "    # Silver Pick: Any 1 of the 3 proposed conditions satisfied, AND RSI >= 55\n",
        "    ticker_df.loc[ticker_df['satisfied_conditions_count'] == 1, 'Recommendation'] = 'Silver Pick'\n",
        "\n",
        "\n",
        "    # Create a description of satisfied conditions\n",
        "    conditions_met = []\n",
        "    if condition_rsi.any(): # Check if there's at least one True in the series\n",
        "         conditions_met.append('RSI >= 55 & Increasing')\n",
        "    if condition_williams.any():\n",
        "        conditions_met.append('Williams %R >= -22 & Increasing')\n",
        "    if condition_macd.any():\n",
        "        conditions_met.append('MACD > Signal & Increasing')\n",
        "    if condition_cci.any(): # Include CCI if you want to describe it when met\n",
        "         conditions_met.append('CCI within thresholds')\n",
        "\n",
        "    # Join the conditions with a comma; handle case where no conditions are met\n",
        "    ticker_df['Satisfied_Conditions_Description'] = ticker_df.apply(\n",
        "        lambda row: ', '.join([cond for cond, met in zip(['RSI >= 55 & Increasing', 'Williams %R >= -22 & Increasing', 'MACD > Signal & Increasing', 'CCI within thresholds'],\n",
        "                                                         [row['RSI'] >= 55 and row['RSI'] > row['RSI_prev'],\n",
        "                                                          row['Williams_%R'] >= -22 and row['Williams_%R'] > row['Williams_%R_prev'],\n",
        "                                                          (row['MACD'] - row['MACD_Signal']) > 0 and row['MACD'] > row['MACD_prev'],\n",
        "                                                          row['cci'] > -200 and row['cci'] < 200]) if met]), axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Filter for rows with 'Diamond Pick', 'Golden Pick', or 'Silver Pick'\n",
        "    filtered_df = ticker_df[(ticker_df['Recommendation'] == 'Diamond Pick') | (ticker_df['Recommendation'] == 'Golden Pick') | (ticker_df['Recommendation'] == 'Silver Pick')]\n",
        "\n",
        "\n",
        "    # Append the filtered data for the current ticker to the result DataFrame\n",
        "    filtered_latest_two_days_df = pd.concat([filtered_latest_two_days_df, filtered_df], ignore_index=True)\n",
        "\n",
        "# Display the tickers, dates, and recommendations\n",
        "display(filtered_latest_two_days_df[['Date', 'Ticker', 'RSI', 'MACD', 'MACD_Signal', 'Williams_%R', 'cci', 'satisfied_conditions_count', 'Recommendation', 'Satisfied_Conditions_Description']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "3TpJQByPFlRz",
        "outputId": "06c0ee9f-de74-4aaa-ff38-64e21b5110bb"
      },
      "outputs": [],
      "source": [
        "combined_tick_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "651d5cfa"
      },
      "source": [
        "# Task\n",
        "Send an email to \"jonnadularohit@gmail.com\" containing the shortlisted tickers from the `filtered_latest_two_days_df` DataFrame and their corresponding technical indicator plots, using the Gmail address and app password stored in Colab secrets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd19329"
      },
      "source": [
        "## Access gmail credentials\n",
        "\n",
        "### Subtask:\n",
        "Securely retrieve the Gmail address and app password from Colab secrets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6434db2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Access the Gmail address and app password from Colab secrets using the `google.colab` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1998546"
      },
      "source": [
        "# Task\n",
        "Backtest the historical performance of the \"Diamond Pick,\" \"Golden Pick,\" and \"Silver Pick\" recommendations based on the indicator filtering logic in cell `0e47eb98`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aced3255"
      },
      "source": [
        "## Define backtesting period\n",
        "\n",
        "### Subtask:\n",
        "Determine the historical date range over which you want to perform the backtesting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab528c87"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine the earliest date after which all technical indicators are calculated in `combined_tick_df` to set the backtesting start date, and set the end date to the latest date in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31t9sGndwzHR",
        "outputId": "93b9e73c-2ead-4010-b1ef-59ab3c1e8312"
      },
      "outputs": [],
      "source": [
        "combined_tick_df[combined_tick_df[\"Date\"] > \"2024-01-01\"].head()\n",
        "print(combined_tick_df.groupby('Ticker')['Date'].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e412b54b"
      },
      "outputs": [],
      "source": [
        "# # Determine the earliest date for each ticker where SMA200 is not NaN\n",
        "# earliest_sma200_dates_per_ticker = combined_tick_df.dropna(subset=['SMA200', 'RSI', 'MFI', 'MACD', 'Williams_%R', 'cci']).groupby('Ticker')['Date'].min()\n",
        "\n",
        "# # Display the earliest date for SMA200 for each ticker\n",
        "# print(\"Earliest Date for SMA200 Availability per Ticker:\")\n",
        "# display(earliest_sma200_dates_per_ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1c20277"
      },
      "outputs": [],
      "source": [
        "# # Determine the earliest date for each ticker after which all indicators have valid data\n",
        "# # The SMA200 has the longest lookback period (200 days)\n",
        "# # We need at least 200 data points for SMA200 to have a value\n",
        "# earliest_dates_per_ticker_with_all_indicators = combined_tick_df.dropna(subset=['SMA200', 'RSI', 'MFI', 'MACD', 'Williams_%R', 'cci']).groupby('Ticker')['Date'].min()\n",
        "\n",
        "# print(\"Earliest Date per Ticker with All Indicators Available:\")\n",
        "# display(earliest_dates_per_ticker_with_all_indicators)\n",
        "\n",
        "# # Determine the overall backtesting start date (the latest of the earliest dates across all tickers)\n",
        "# earliest_date_with_all_indicators = earliest_dates_per_ticker_with_all_indicators.max()\n",
        "\n",
        "\n",
        "# # Determine the latest date in the data\n",
        "# latest_date = combined_tick_df['Date'].max()\n",
        "\n",
        "# print(f\"\\nBacktesting start date (latest of the above): {earliest_date_with_all_indicators.strftime('%Y-%m-%d')}\")\n",
        "# print(f\"Backtesting end date: {latest_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# # Define the backtesting period\n",
        "# backtesting_dates = combined_tick_df[(combined_tick_df['Date'] >= earliest_date_with_all_indicators) & (combined_tick_df['Date'] <= latest_date)]['Date'].unique()\n",
        "# backtesting_dates = sorted(backtesting_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73796b4"
      },
      "outputs": [],
      "source": [
        "# # Filter the Series to find tickers with earliest date in 2025\n",
        "# tickers_with_earliest_date_in_2025 = earliest_dates_per_ticker_with_all_indicators[\n",
        "#     (earliest_dates_per_ticker_with_all_indicators.dt.year == 2025)\n",
        "# ]\n",
        "\n",
        "# # Display the tickers and their earliest dates in 2025\n",
        "# print(\"Tickers with Earliest Date (All Indicators Available) in 2025:\")\n",
        "# display(tickers_with_earliest_date_in_2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e56f15b7",
        "outputId": "c14cdb39-3502-4b92-d786-26de10fc1ca9"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store historical picks\n",
        "historical_picks = []\n",
        "\n",
        "# Determine the overall latest date in the data (used as the end date for all tickers' backtesting)\n",
        "overall_latest_date = combined_tick_df['Date'].max()\n",
        "\n",
        "# Get the earliest date with all indicators available for each ticker (reusing logic from d1c20277)\n",
        "earliest_dates_per_ticker_with_all_indicators = combined_tick_df.dropna(subset=['SMA200', 'RSI', 'MFI', 'MACD', 'Williams_%R', 'cci']).groupby('Ticker')['Date'].min()\n",
        "\n",
        "\n",
        "# Iterate through each unique ticker\n",
        "for ticker in combined_tick_df['Ticker'].unique():\n",
        "    # Get the earliest date with all indicators available for the current ticker\n",
        "    # Handle cases where a ticker might not have data for all indicators (though dropna in combined_tick_df should prevent this)\n",
        "    if ticker in earliest_dates_per_ticker_with_all_indicators.index:\n",
        "        ticker_backtesting_start_date = earliest_dates_per_ticker_with_all_indicators.loc[ticker]\n",
        "    else:\n",
        "        # If for some reason a ticker doesn't appear in the earliest_dates_per_ticker_with_all_indicators, skip it\n",
        "        print(f\"Skipping ticker {ticker}: No data with all indicators available.\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    # Filter data for the current ticker within its specific backtesting period\n",
        "    ticker_historical_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                               (combined_tick_df['Date'] >= ticker_backtesting_start_date) &\n",
        "                                               (combined_tick_df['Date'] <= overall_latest_date)].copy()\n",
        "\n",
        "    # Sort by date to ensure correct previous day calculations\n",
        "    ticker_historical_data = ticker_historical_data.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # Apply the filtering logic and categorization for each day in the ticker's backtesting period\n",
        "    # Iterate through the data for the current ticker, starting from the second day\n",
        "    # to allow for previous day comparison\n",
        "    for i in range(1, len(ticker_historical_data)):\n",
        "        current_day_data = ticker_historical_data.iloc[i]\n",
        "        previous_day_data = ticker_historical_data.iloc[i-1]\n",
        "        current_date = current_day_data['Date']\n",
        "\n",
        "        # Calculate previous day's values\n",
        "        current_day_data['RSI_prev'] = previous_day_data['RSI']\n",
        "        current_day_data['MACD_prev'] = previous_day_data['MACD']\n",
        "        current_day_data['Williams_%R_prev'] = previous_day_data['Williams_%R']\n",
        "        current_day_data['cci_prev'] = previous_day_data['cci']\n",
        "\n",
        "\n",
        "        # Define the filtering conditions based on the logic in cell 0e47eb98\n",
        "        # Ensure previous day data is not NaN for the comparison\n",
        "        condition_rsi = (current_day_data['RSI'] >= 55) and \\\n",
        "                        (not pd.isna(current_day_data['RSI_prev'])) and \\\n",
        "                        (current_day_data['RSI'] > current_day_data['RSI_prev'])\n",
        "\n",
        "        condition_williams = (current_day_data['Williams_%R'] >= -22) and \\\n",
        "                             (not pd.isna(current_day_data['Williams_%R_prev'])) and \\\n",
        "                             (current_day_data['Williams_%R'] > current_day_data['Williams_%R_prev'])\n",
        "\n",
        "        condition_macd = ((current_day_data['MACD'] - current_day_data['MACD_Signal']) > 0) and \\\n",
        "                         (not pd.isna(current_day_data['MACD_prev'])) and \\\n",
        "                         (current_day_data['MACD'] > current_day_data['MACD_prev'])\n",
        "\n",
        "        condition_cci = (current_day_data['cci'] > -200) and (current_day_data['cci'] < 200)\n",
        "\n",
        "\n",
        "        # Count how many of the *three proposed* conditions are satisfied\n",
        "        satisfied_conditions_count = int(condition_rsi) + \\\n",
        "                                       int(condition_williams) + \\\n",
        "                                       int(condition_macd)\n",
        "\n",
        "        # Determine the recommendation based on the count (excluding CCI for categorization as per original logic)\n",
        "        recommendation = 'None'\n",
        "        if satisfied_conditions_count == 3:\n",
        "            recommendation = 'Diamond Pick'\n",
        "        elif satisfied_conditions_count == 2:\n",
        "            recommendation = 'Golden Pick'\n",
        "        elif satisfied_conditions_count == 1 and condition_rsi: # Silver Pick: Any 1 of the 3 proposed conditions satisfied, AND RSI >= 55\n",
        "             recommendation = 'Silver Pick'\n",
        "        # Re-evaluating Silver Pick logic based on cell 0e47eb98: \"Any 1 of the 3 proposed conditions satisfied, AND RSI >= 55\"\n",
        "        # The original code in 0e47eb98 didn't check for RSI >= 55 *again* for the Silver Pick after counting satisfied conditions.\n",
        "        # It seemed the intention was just based on the count of the 3 proposed conditions.\n",
        "        # Let's stick to the count logic for categorization as it was implemented in 0e47eb98\n",
        "        recommendation = 'None'\n",
        "        if satisfied_conditions_count == 3:\n",
        "            recommendation = 'Diamond Pick'\n",
        "        elif satisfied_conditions_count == 2:\n",
        "            recommendation = 'Golden Pick'\n",
        "        elif satisfied_conditions_count == 1: # Silver Pick: Any 1 of the 3 proposed conditions satisfied\n",
        "             recommendation = 'Silver Pick'\n",
        "\n",
        "\n",
        "        # Create a description of satisfied conditions for this specific day's trigger\n",
        "        conditions_met_description = []\n",
        "        if condition_rsi:\n",
        "             conditions_met_description.append('RSI >= 55 & Increasing')\n",
        "        if condition_williams:\n",
        "            conditions_met_description.append('Williams %R >= -22 & Increasing')\n",
        "        if condition_macd:\n",
        "            conditions_met_description.append('MACD > Signal & Increasing')\n",
        "        if condition_cci: # Include CCI in description if it meets its condition\n",
        "             conditions_met_description.append('CCI within thresholds')\n",
        "\n",
        "        satisfied_conditions_text = ', '.join(conditions_met_description) if conditions_met_description else 'None'\n",
        "\n",
        "\n",
        "        # Record the pick if it's not 'None'\n",
        "        if recommendation != 'None':\n",
        "            historical_picks.append({\n",
        "                'Date': current_date,\n",
        "                'Ticker': ticker,\n",
        "                'Recommendation': recommendation,\n",
        "                'satisfied_conditions_count': satisfied_conditions_count,\n",
        "                'Close_Price_on_Pick_Date': current_day_data['Close'],\n",
        "                'RSI': current_day_data['RSI'], # Include indicator values for context\n",
        "                'MACD': current_day_data['MACD'],\n",
        "                'MACD_Signal': current_day_data['MACD_Signal'],\n",
        "                'Williams_%R': current_day_data['Williams_%R'],\n",
        "                'cci': current_day_data['cci'],\n",
        "                'Satisfied_Conditions_Description': satisfied_conditions_text\n",
        "            })\n",
        "\n",
        "\n",
        "# Convert the list of historical picks to a DataFrame\n",
        "historical_picks_df = pd.DataFrame(historical_picks)\n",
        "\n",
        "# Display the historical picks\n",
        "print(\"Historical Picks (Ticker-Specific Backtesting):\")\n",
        "display(historical_picks_df.head())\n",
        "print(\"\\nTotal Historical Picks:\")\n",
        "display(historical_picks_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b8916ee"
      },
      "source": [
        "# Task\n",
        "Visualize the cumulative returns of each recommendation type based on the historical picks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27eba1b8"
      },
      "source": [
        "## Calculate returns for historical picks\n",
        "\n",
        "### Subtask:\n",
        "For each historical pick in `historical_picks_df`, determine the price change or return over a defined period following the pick date.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13b19f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the returns for each historical pick over a defined holding period by finding the closing price on the pick date and the price after the holding period.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea4f2b1",
        "outputId": "f48cd42d-0c5e-4d8d-fdb2-7413e11050a2"
      },
      "outputs": [],
      "source": [
        "# 1. Define the holding period\n",
        "holding_period = 5 # Example: 5 trading days (approximately one week)\n",
        "\n",
        "# Initialize columns for return and target date\n",
        "historical_picks_df['Return'] = np.nan\n",
        "historical_picks_df['Max_Price_in_Holding_Period'] = np.nan # New column for max price\n",
        "historical_picks_df['Date_of_Max_Price'] = pd.NaT # New column for date of max price\n",
        "\n",
        "\n",
        "# 2. Iterate through each historical pick to calculate returns\n",
        "for index, row in historical_picks_df.iterrows():\n",
        "    ticker = row['Ticker']\n",
        "    pick_date = row['Date']\n",
        "    price_on_pick_date = row['Close_Price_on_Pick_Date']\n",
        "\n",
        "    # Calculate the end date for the holding period\n",
        "    end_date_holding_period = pick_date + pd.Timedelta(days=holding_period)\n",
        "\n",
        "    # Find the data for the specific ticker within the holding period (starting the day after the pick)\n",
        "    holding_period_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                           (combined_tick_df['Date'] > pick_date) &\n",
        "                                           (combined_tick_df['Date'] <= end_date_holding_period)].copy()\n",
        "\n",
        "    # Ensure there is data in the holding period\n",
        "    if not holding_period_data.empty:\n",
        "        # Find the maximum 'High' price within the holding period\n",
        "        max_price = holding_period_data['High'].max()\n",
        "        # Find the date of the maximum price\n",
        "        date_of_max_price = holding_period_data.loc[holding_period_data['High'].idxmax(), 'Date']\n",
        "\n",
        "\n",
        "        # Calculate the return based on the maximum price, and multiply by 100 for percentage\n",
        "        calculated_return = ((max_price - price_on_pick_date) / price_on_pick_date) * 100\n",
        "\n",
        "        # Store the calculated return, max price, and date of max price\n",
        "        historical_picks_df.loc[index, 'Return'] = calculated_return\n",
        "        historical_picks_df.loc[index, 'Max_Price_in_Holding_Period'] = max_price\n",
        "        historical_picks_df.loc[index, 'Date_of_Max_Price'] = date_of_max_price\n",
        "\n",
        "\n",
        "# Display the updated DataFrame with returns and target dates\n",
        "display(historical_picks_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "163fe1a4",
        "outputId": "2a097b9d-9b43-43bb-b4f7-61730f121ced"
      },
      "outputs": [],
      "source": [
        "# Analyze performance metrics for each recommendation type\n",
        "\n",
        "# performance_metrics_by_recommendation = {}\n",
        "\n",
        "# for recommendation_type in historical_picks_df['Recommendation'].unique():\n",
        "#     category_picks = historical_picks_df[historical_picks_df['Recommendation'] == recommendation_type].copy()\n",
        "\n",
        "#     if not category_picks.empty:\n",
        "#         total_picks = len(category_picks)\n",
        "#         winning_picks = category_picks[category_picks['Return'] > 0]\n",
        "#         losing_picks = category_picks[category_picks['Return'] <= 0] # Consider 0 return as not a win\n",
        "\n",
        "#         win_rate = len(winning_picks) / total_picks if total_picks > 0 else 0\n",
        "#         loss_rate = len(losing_picks) / total_picks if total_picks > 0 else 0\n",
        "\n",
        "#         average_winning_return = winning_picks['Return'].mean() if not winning_picks.empty else 0\n",
        "#         average_losing_return = losing_picks['Return'].mean() if not losing_picks.empty else 0\n",
        "#         average_total_return = category_picks['Return'].mean()\n",
        "\n",
        "\n",
        "#         performance_metrics_by_recommendation[recommendation_type] = {\n",
        "#             'Total Picks': total_picks,\n",
        "#             'Winning Picks': len(winning_picks),\n",
        "#             'Losing Picks': len(losing_picks),\n",
        "#             'Win Rate (%)': win_rate * 100,\n",
        "#             'Loss Rate (%)': loss_rate * 100,\n",
        "#             'Average Winning Return (%)': average_winning_return, # Removed * 100\n",
        "#             'Average Losing Return (%)': average_losing_return, # Removed * 100\n",
        "#             'Average Total Return (%)': average_total_return # Removed * 100\n",
        "#         }\n",
        "\n",
        "# # Convert the performance metrics dictionary to a DataFrame for better display\n",
        "# performance_metrics_by_recommendation_df = pd.DataFrame.from_dict(performance_metrics_by_recommendation, orient='index')\n",
        "\n",
        "# # Display the performance metrics by recommendation type\n",
        "# print(\"Performance Metrics by Recommendation Type:\")\n",
        "# display(performance_metrics_by_recommendation_df)\n",
        "\n",
        "\n",
        "# Analyze performance metrics for each ticker\n",
        "# performance_metrics_by_ticker = {}\n",
        "\n",
        "# for ticker in historical_picks_df['Ticker'].unique():\n",
        "#     ticker_picks = historical_picks_df[historical_picks_df['Ticker'] == ticker].copy()\n",
        "\n",
        "#     if not ticker_picks.empty:\n",
        "#         total_picks = len(ticker_picks)\n",
        "#         winning_picks = ticker_picks[ticker_picks['Return'] > 0]\n",
        "#         losing_picks = ticker_picks[ticker_picks['Return'] <= 0] # Consider 0 return as not a win\n",
        "\n",
        "#         win_rate = len(winning_picks) / total_picks if total_picks > 0 else 0\n",
        "#         loss_rate = len(losing_picks) / total_picks if total_picks > 0 else 0\n",
        "\n",
        "#         average_winning_return = winning_picks['Return'].mean() if not winning_picks.empty else 0\n",
        "#         average_losing_return = losing_picks['Return'].mean() if not losing_picks.empty else 0\n",
        "#         average_total_return = ticker_picks['Return'].mean()\n",
        "\n",
        "\n",
        "#         performance_metrics_by_ticker[ticker] = {\n",
        "#             'Total Picks': total_picks,\n",
        "#             'Winning Picks': len(winning_picks),\n",
        "#             'Losing Picks': len(losing_picks),\n",
        "#             'Win Rate (%)': win_rate * 100,\n",
        "#             'Loss Rate (%)': loss_rate * 100,\n",
        "#             'Average Winning Return (%)': average_winning_return, # Removed * 100\n",
        "#             'Average Losing Return (%)': average_losing_return, # Removed * 100\n",
        "#             'Average Total Return (%)': average_total_return # Removed * 100\n",
        "#         }\n",
        "\n",
        "# # Convert the performance metrics dictionary to a DataFrame for better display\n",
        "# performance_metrics_by_ticker_df = pd.DataFrame.from_dict(performance_metrics_by_ticker, orient='index')\n",
        "\n",
        "# # Display the performance metrics by ticker\n",
        "# print(\"\\nPerformance Metrics by Ticker:\")\n",
        "# display(performance_metrics_by_ticker_df)\n",
        "\n",
        "\n",
        "# Analyze performance metrics grouped by Ticker and Recommendation Type\n",
        "performance_metrics_by_ticker_recommendation = {}\n",
        "\n",
        "# Add a 'Year' column to historical_picks_df for grouping\n",
        "historical_picks_df['Year'] = historical_picks_df['Date'].dt.year\n",
        "\n",
        "# Group by Ticker, Recommendation Type, and Year\n",
        "grouped_performance = historical_picks_df.groupby(['Ticker', 'Recommendation', 'Year'])\n",
        "\n",
        "for name, group in grouped_performance:\n",
        "    ticker = name[0]\n",
        "    recommendation_type = name[1]\n",
        "    year = name[2]\n",
        "    category_picks = group.copy() # Use the group as the category_picks\n",
        "\n",
        "    if not category_picks.empty:\n",
        "        total_picks = len(category_picks)\n",
        "        winning_picks = category_picks[category_picks['Return'] > 0]\n",
        "        losing_picks = category_picks[category_picks['Return'] <= 0] # Consider 0 return as not a win\n",
        "\n",
        "        win_rate = len(winning_picks) / total_picks if total_picks > 0 else 0\n",
        "        loss_rate = len(losing_picks) / total_picks if total_picks > 0 else 0\n",
        "\n",
        "        average_winning_return = winning_picks['Return'].mean() if not winning_picks.empty else 0\n",
        "        average_losing_return = losing_picks['Return'].mean() if not losing_picks.empty else 0\n",
        "        average_total_return = category_picks['Return'].mean()\n",
        "\n",
        "        # Store metrics using a tuple (ticker, recommendation_type, year) as the key\n",
        "        performance_metrics_by_ticker_recommendation[(ticker, recommendation_type, year)] = {\n",
        "            'Total Picks': total_picks,\n",
        "            'Winning Picks': len(winning_picks),\n",
        "            'Losing Picks': len(losing_picks),\n",
        "            'Win Rate (%)': win_rate * 100,\n",
        "            'Loss Rate (%)': loss_rate * 100,\n",
        "            'Average Winning Return (%)': average_winning_return, # Removed * 100\n",
        "            'Average Losing Return (%)': average_losing_return, # Removed * 100\n",
        "            'Average Total Return (%)': average_total_return # Removed * 100\n",
        "        }\n",
        "\n",
        "# Convert the performance metrics dictionary to a DataFrame for better display\n",
        "performance_metrics_by_ticker_recommendation_df = pd.DataFrame.from_dict(performance_metrics_by_ticker_recommendation, orient='index')\n",
        "\n",
        "# Rename the index for clarity\n",
        "performance_metrics_by_ticker_recommendation_df.index.names = ['Ticker', 'Recommendation', 'Year']\n",
        "\n",
        "\n",
        "# Display the performance metrics grouped by Ticker and Recommendation Type\n",
        "print(\"\\nPerformance Metrics by Ticker, Recommendation Type, and Year:\")\n",
        "display(performance_metrics_by_ticker_recommendation_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aaf326"
      },
      "source": [
        "## Backtesting Analysis for New Signal Conditions\n",
        "\n",
        "### Subtask:\n",
        "Perform backtesting analysis for `signal_1`, `signal_2`, and `signal_3` conditions and calculate the winning percentage for each signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd3d29c"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through historical dates, apply the new complex signal conditions, record triggers, calculate returns after triggers, and summarize winning percentages per signal over the backtesting period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de73b6d9",
        "outputId": "7c1d5779-01f1-4a5d-942f-7bbc6102a8a4"
      },
      "outputs": [],
      "source": [
        "# Define the holding period for performance analysis after a signal trigger\n",
        "holding_period_signals = 5 # Example: 5 trading days\n",
        "\n",
        "# Initialize lists to store signal triggers and their performance\n",
        "signal_triggers = {\n",
        "    'signal_1': [],\n",
        "    'signal_2': [],\n",
        "    'signal_3': []\n",
        "}\n",
        "\n",
        "# Determine the overall latest date in the data\n",
        "overall_latest_date = combined_tick_df['Date'].max()\n",
        "\n",
        "# Get the earliest date with all indicators available for each ticker (reusing logic from d1c20277)\n",
        "earliest_dates_per_ticker_with_all_indicators = combined_tick_df.dropna(subset=['SMA200', 'RSI', 'MFI', 'MACD', 'Williams_%R', 'cci']).groupby('Ticker')['Date'].min()\n",
        "\n",
        "\n",
        "# Iterate through each unique ticker\n",
        "for ticker in combined_tick_df['Ticker'].unique():\n",
        "    # Get the earliest date with all indicators available for the current ticker\n",
        "    if ticker in earliest_dates_per_ticker_with_all_indicators.index:\n",
        "        ticker_backtesting_start_date = earliest_dates_per_ticker_with_all_indicators.loc[ticker]\n",
        "    else:\n",
        "        # If for some reason a ticker doesn't appear in the earliest_dates_per_ticker_with_all_indicators, skip it\n",
        "        print(f\"Skipping ticker {ticker}: No data with all indicators available for backtesting new signals.\")\n",
        "        continue\n",
        "\n",
        "    # Filter data for the current ticker within its specific backtesting period\n",
        "    ticker_historical_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                               (combined_tick_df['Date'] >= ticker_backtesting_start_date) &\n",
        "                                               (combined_tick_df['Date'] <= overall_latest_date)].copy()\n",
        "\n",
        "    # Sort by date to ensure correct previous day calculations\n",
        "    ticker_historical_data = ticker_historical_data.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # Iterate through the data for the current ticker, starting from the second day\n",
        "    # to allow for previous day comparison and rolling calculations\n",
        "    for i in range(1, len(ticker_historical_data)):\n",
        "        current_day_data = ticker_historical_data.iloc[i]\n",
        "        previous_day_data = ticker_historical_data.iloc[i-1]\n",
        "        current_date = current_day_data['Date']\n",
        "\n",
        "        # Ensure previous day data is not NaN for comparison\n",
        "        if pd.isna(previous_day_data['RSI']) or pd.isna(previous_day_data['MACD']) or pd.isna(previous_day_data['Williams_%R']) or pd.isna(previous_day_data['cci']):\n",
        "             continue # Skip if previous day indicators are not available\n",
        "\n",
        "        # Calculate rolling 20-day volume mean up to the current date\n",
        "        # Ensure there are enough data points for the rolling mean (at least 20 including current day)\n",
        "        if i >= 19: # Need at least 20 data points (index 0 to 19) for rolling window 20\n",
        "             rolling_vol_mean_20 = ticker_historical_data.loc[i-19:i, 'Volume'].mean() # Calculate rolling mean up to current day\n",
        "        else:\n",
        "             rolling_vol_mean_20 = np.nan # Not enough data for rolling mean\n",
        "\n",
        "\n",
        "        # Define the conditions based on the user's signal definitions\n",
        "        rsi = current_day_data['RSI']\n",
        "        williams_r = current_day_data['Williams_%R']\n",
        "        macd = current_day_data['MACD']\n",
        "        macd_signal = current_day_data['MACD_Signal']\n",
        "        cci = current_day_data['cci']\n",
        "        obv = current_day_data['OBV']\n",
        "        prev_obv = previous_day_data['OBV']\n",
        "        volume = current_day_data['Volume']\n",
        "\n",
        "\n",
        "        # Define signal_1\n",
        "        signal_1_triggered = (\n",
        "            (55 <= rsi <= 60) and\n",
        "            (-22 <= williams_r <= -16) and\n",
        "            (macd > macd_signal) and\n",
        "            (macd > 0) and (\n",
        "                (-200 < cci < 200) or # Changed from and to or\n",
        "                (obv > prev_obv) or # Changed from and to or\n",
        "                (volume > 1.5 * rolling_vol_mean_20 if not pd.isna(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Define signal_2\n",
        "        signal_2_triggered = (\n",
        "            (60 < rsi <= 65) and\n",
        "            (-20 < williams_r <= -9) and # Adjusted range based on user's definition\n",
        "            (macd > macd_signal) and\n",
        "            (macd > 0) and (\n",
        "                (-200 < cci < 200) or # Changed from and to or\n",
        "                (obv > prev_obv) or # Changed from and to or\n",
        "                (volume > 1.5 * rolling_vol_mean_20 if not pd.isna(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "             )\n",
        "        )\n",
        "\n",
        "        # Define signal_3\n",
        "        signal_3_triggered = (\n",
        "            (65 < rsi <= 70) and\n",
        "            (-16 < williams_r <= -2) and # Adjusted range based on user's definition\n",
        "            (macd > macd_signal) and\n",
        "            (macd > 0) and (\n",
        "                (-200 < cci < 200) or # Changed from and to or\n",
        "                (obv > prev_obv) or # Changed from and to or\n",
        "                (volume > 1.5 * rolling_vol_mean_20 if not pd.isna(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        # Record triggered signals and calculate performance\n",
        "        if signal_1_triggered:\n",
        "            trigger_info = {\n",
        "                'Date': current_date,\n",
        "                'Ticker': ticker,\n",
        "                'Close_Price_on_Trigger_Date': current_day_data['Close']\n",
        "            }\n",
        "            # Calculate performance for this trigger\n",
        "            price_on_trigger_date = trigger_info['Close_Price_on_Trigger_Date']\n",
        "            # Find the data for the specific ticker within the holding period (starting the day after the trigger)\n",
        "            holding_period_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                                    (combined_tick_df['Date'] > current_date) &\n",
        "                                                    (combined_tick_df['Date'] <= current_date + pd.Timedelta(days=holding_period_signals))].copy()\n",
        "\n",
        "            winning_trigger = False\n",
        "            if not holding_period_data.empty:\n",
        "                # Find the maximum 'High' price within the holding period\n",
        "                max_price = holding_period_data['High'].max()\n",
        "                # Check if the maximum price is higher than the trigger price (positive return)\n",
        "                if max_price > price_on_trigger_date:\n",
        "                    winning_trigger = True\n",
        "\n",
        "            trigger_info['Winning_Trigger'] = winning_trigger\n",
        "            signal_triggers['signal_1'].append(trigger_info)\n",
        "\n",
        "\n",
        "        if signal_2_triggered:\n",
        "             trigger_info = {\n",
        "                'Date': current_date,\n",
        "                'Ticker': ticker,\n",
        "                'Close_Price_on_Trigger_Date': current_day_data['Close']\n",
        "            }\n",
        "             # Calculate performance for this trigger\n",
        "             price_on_trigger_date = trigger_info['Close_Price_on_Trigger_Date']\n",
        "             holding_period_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                                    (combined_tick_df['Date'] > current_date) &\n",
        "                                                    (combined_tick_df['Date'] <= current_date + pd.Timedelta(days=holding_period_signals))].copy()\n",
        "             winning_trigger = False\n",
        "             if not holding_period_data.empty:\n",
        "                max_price = holding_period_data['High'].max()\n",
        "                if max_price > price_on_trigger_date:\n",
        "                    winning_trigger = True\n",
        "\n",
        "             trigger_info['Winning_Trigger'] = winning_trigger\n",
        "             signal_triggers['signal_2'].append(trigger_info)\n",
        "\n",
        "\n",
        "        if signal_3_triggered:\n",
        "             trigger_info = {\n",
        "                'Date': current_date,\n",
        "                'Ticker': ticker,\n",
        "                'Close_Price_on_Trigger_Date': current_day_data['Close']\n",
        "            }\n",
        "             # Calculate performance for this trigger\n",
        "             price_on_trigger_date = trigger_info['Close_Price_on_Trigger_Date']\n",
        "             holding_period_data = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                                    (combined_tick_df['Date'] > current_date) &\n",
        "                                                    (combined_tick_df['Date'] <= current_date + pd.Timedelta(days=holding_period_signals))].copy()\n",
        "             winning_trigger = False\n",
        "             if not holding_period_data.empty:\n",
        "                max_price = holding_period_data['High'].max()\n",
        "                if max_price > price_on_trigger_date:\n",
        "                    winning_trigger = True\n",
        "\n",
        "             trigger_info['Winning_Trigger'] = winning_trigger\n",
        "             signal_triggers['signal_3'].append(trigger_info)\n",
        "\n",
        "\n",
        "# Convert signal triggers lists to DataFrames\n",
        "signal_triggers_df = {}\n",
        "for signal, triggers in signal_triggers.items():\n",
        "    signal_triggers_df[signal] = pd.DataFrame(triggers)\n",
        "\n",
        "# Calculate performance (winning percentage) for each signal, grouped by ticker\n",
        "signal_performance_by_ticker = {}\n",
        "\n",
        "for signal, df in signal_triggers_df.items():\n",
        "    signal_performance_by_ticker[signal] = {} # Initialize nested dictionary for this signal\n",
        "\n",
        "    if not df.empty:\n",
        "        # Group triggers by ticker for this signal\n",
        "        grouped_triggers = df.groupby('Ticker')\n",
        "\n",
        "        for ticker, ticker_triggers_df in grouped_triggers:\n",
        "            total_triggers = len(ticker_triggers_df)\n",
        "            winning_triggers = ticker_triggers_df['Winning_Trigger'].sum()\n",
        "\n",
        "            # Calculate winning percentage for this ticker and signal\n",
        "            winning_percentage = (winning_triggers / total_triggers) * 100 if total_triggers > 0 else 0\n",
        "\n",
        "            # Store performance for this ticker and signal\n",
        "            signal_performance_by_ticker[signal][ticker] = {\n",
        "                'Total Triggers': total_triggers,\n",
        "                'Winning Triggers (based on Max Price)': winning_triggers,\n",
        "                'Winning Percentage (%)': winning_percentage\n",
        "            }\n",
        "    else:\n",
        "         # If no triggers for this signal, add empty entry for each ticker that has enough data for backtesting\n",
        "        tickers_with_data = earliest_dates_per_ticker_with_all_indicators.index.tolist()\n",
        "        for ticker in tickers_with_data:\n",
        "             signal_performance_by_ticker[signal][ticker] = {\n",
        "                'Total Triggers': 0,\n",
        "                'Winning Triggers (based on Max Price)': 0,\n",
        "                'Winning Percentage (%)': 0\n",
        "            }\n",
        "\n",
        "\n",
        "# Convert the nested dictionary to a DataFrame for display\n",
        "# Create a list of tuples for the MultiIndex\n",
        "index_tuples = [(signal, ticker) for signal, ticker_data in signal_performance_by_ticker.items() for ticker in ticker_data.keys()]\n",
        "# Create a list of dictionaries for the data\n",
        "data_list = [metrics for signal, ticker_data in signal_performance_by_ticker.items() for metrics in ticker_data.values()]\n",
        "\n",
        "if index_tuples:\n",
        "    signal_performance_by_ticker_df = pd.DataFrame(data_list, index=pd.MultiIndex.from_tuples(index_tuples, names=['Signal', 'Ticker']))\n",
        "else:\n",
        "    # Create an empty DataFrame with the correct columns if no triggers were found for any signal/ticker\n",
        "    signal_performance_by_ticker_df = pd.DataFrame(columns=['Total Triggers', 'Winning Triggers (based on Max Price)', 'Winning Percentage (%)'])\n",
        "\n",
        "\n",
        "# Display the winning percentages for each signal, grouped by ticker\n",
        "print(\"Winning Percentage for New Signal Conditions (Grouped by Ticker):\")\n",
        "display(signal_performance_by_ticker_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83a9e98"
      },
      "source": [
        "## Identify Latest New Signals Triggered by Shortlisted Tickers\n",
        "\n",
        "### Subtask:\n",
        "Check which of the new signal conditions (`signal_1`, `signal_2`, `signal_3`) were triggered by the shortlisted tickers on the latest date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76979c40"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the shortlisted tickers from `filtered_latest_two_days_df`, find the latest date for each, and apply the new signal conditions to determine which signals were triggered on that specific date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ee183ae",
        "outputId": "a3692dae-98de-4880-d9ac-813156637789"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store which new signals were triggered for each shortlisted ticker on the latest date\n",
        "latest_new_signals_for_shortlisted = {}\n",
        "\n",
        "if not filtered_latest_two_days_df.empty:\n",
        "    for ticker in filtered_latest_two_days_df['Ticker'].unique():\n",
        "        # Get the latest data point for this shortlisted ticker from the filtered DataFrame\n",
        "        latest_filtered_data = filtered_latest_two_days_df[filtered_latest_two_days_df['Ticker'] == ticker].iloc[-1]\n",
        "        latest_date = latest_filtered_data['Date']\n",
        "\n",
        "        # Find the corresponding data for the latest date and the previous day from the full combined_tick_df\n",
        "        # This ensures we have enough historical data for indicators and previous day comparisons\n",
        "        ticker_full_data_up_to_latest_date = combined_tick_df[(combined_tick_df['Ticker'] == ticker) &\n",
        "                                                              (combined_tick_df['Date'] <= latest_date)].copy()\n",
        "\n",
        "        if len(ticker_full_data_up_to_latest_date) > 1: # Need at least two days for previous day comparison\n",
        "            current_day_data = ticker_full_data_up_to_latest_date.iloc[-1]\n",
        "            previous_day_data = ticker_full_data_up_to_latest_date.iloc[-2]\n",
        "\n",
        "            # Calculate rolling 20-day volume mean up to the latest date\n",
        "            # Ensure there are enough data points for the rolling mean\n",
        "            if len(ticker_full_data_up_to_latest_date) >= 20: # Need at least 20 data points including the current day\n",
        "                 rolling_vol_mean_20 = ticker_full_data_up_to_latest_date['Volume'].rolling(window=20).mean().iloc[-1]\n",
        "            else:\n",
        "                 rolling_vol_mean_20 = np.nan # Not enough data for rolling mean\n",
        "\n",
        "\n",
        "            # Define the conditions based on the user's signal definitions (replicated from de73b6d9)\n",
        "            rsi = current_day_data['RSI']\n",
        "            williams_r = current_day_data['Williams_%R']\n",
        "            macd = current_day_data['MACD']\n",
        "            macd_signal = current_day_data['MACD_Signal']\n",
        "            cci = current_day_data['cci']\n",
        "            obv = current_day_data['OBV']\n",
        "            prev_obv = previous_day_data['OBV']\n",
        "            volume = current_day_data['Volume']\n",
        "\n",
        "\n",
        "            # Define signal_1, signal_2, signal_3 triggered on the latest day (using corrected logic with ORs)\n",
        "            signal_1_triggered = (\n",
        "                (55 <= rsi <= 60) and\n",
        "                (-22 <= williams_r <= -16) and\n",
        "                (macd > macd_signal) and\n",
        "                (macd > 0) and (\n",
        "                    (-200 < cci < 200) or\n",
        "                    (obv > prev_obv) or\n",
        "                    (volume > 1.5 * rolling_vol_mean_20 if not np.isnan(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "                )\n",
        "            )\n",
        "\n",
        "            signal_2_triggered = (\n",
        "                (60 < rsi <= 65) and\n",
        "                (-20 < williams_r <= -9) and\n",
        "                (macd > macd_signal) and\n",
        "                (macd > 0) and (\n",
        "                    (-200 < cci < 200) or\n",
        "                    (obv > prev_obv) or\n",
        "                    (volume > 1.5 * rolling_vol_mean_20 if not np.isnan(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "                 )\n",
        "            )\n",
        "\n",
        "            signal_3_triggered = (\n",
        "                (65 < rsi <= 70) and\n",
        "                (-16 < williams_r <= -2) and\n",
        "                (macd > macd_signal) and\n",
        "                (macd > 0) and (\n",
        "                    (-200 < cci < 200) or\n",
        "                    (obv > prev_obv) or\n",
        "                    (volume > 1.5 * rolling_vol_mean_20 if not np.isnan(rolling_vol_mean_20) else False) # Handle potential NaN in rolling mean\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "            triggered_signals_list = []\n",
        "            if signal_1_triggered:\n",
        "                triggered_signals_list.append('Signal 1')\n",
        "            if signal_2_triggered:\n",
        "                triggered_signals_list.append('Signal 2')\n",
        "            if signal_3_triggered:\n",
        "                triggered_signals_list.append('Signal 3')\n",
        "\n",
        "            latest_new_signals_for_shortlisted[ticker] = ', '.join(triggered_signals_list) if triggered_signals_list else 'None of the new signals triggered'\n",
        "        else:\n",
        "            latest_new_signals_for_shortlisted[ticker] = 'Not enough data for signal check'\n",
        "\n",
        "# Display the latest new signals triggered for each shortlisted ticker\n",
        "print(\"Latest New Signals Triggered by Shortlisted Tickers:\")\n",
        "if latest_new_signals_for_shortlisted:\n",
        "    for ticker, signals in latest_new_signals_for_shortlisted.items():\n",
        "        print(f\"{ticker}: {signals}\")\n",
        "else:\n",
        "    print(\"No shortlisted tickers found or not enough data to check signals.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff122d88"
      },
      "source": [
        "## Calculate daily returns for each category\n",
        "\n",
        "### Subtask:\n",
        "Aggregate the returns of the picks for each day and each recommendation type to get daily portfolio returns for each category.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a15873"
      },
      "source": [
        "**Reasoning**:\n",
        "Aggregate the returns by date and recommendation type and pivot the data for cumulative return calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad0a33d0"
      },
      "outputs": [],
      "source": [
        "# # Group by Date and Recommendation and calculate the mean return\n",
        "# daily_returns = historical_picks_df.groupby(['Date', 'Recommendation'])['Return'].mean().reset_index()\n",
        "\n",
        "# # Pivot the DataFrame to have Dates as index and Recommendation types as columns\n",
        "# daily_portfolio_returns = daily_returns.pivot(index='Date', columns='Recommendation', values='Return')\n",
        "\n",
        "# # Fill missing values with 0\n",
        "# daily_portfolio_returns = daily_portfolio_returns.fillna(0)\n",
        "\n",
        "# # Sort the DataFrame by date\n",
        "# daily_portfolio_returns = daily_portfolio_returns.sort_index()\n",
        "\n",
        "# # Display the daily portfolio returns\n",
        "# display(daily_portfolio_returns.head())\n",
        "# display(daily_portfolio_returns.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694902a1"
      },
      "source": [
        "## Calculate cumulative returns\n",
        "\n",
        "### Subtask:\n",
        "Calculate the cumulative returns for each recommendation type over the backtesting period.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882a7a50"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the cumulative returns for each recommendation category by adding 1 to the daily return and then calculating the cumulative product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8b167f8"
      },
      "outputs": [],
      "source": [
        "# # Calculate cumulative returns\n",
        "# cumulative_returns = (daily_portfolio_returns + 1).cumprod()\n",
        "\n",
        "# # Display the head of the cumulative returns DataFrame\n",
        "# display(cumulative_returns.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738ecec8"
      },
      "source": [
        "## Visualize cumulative returns\n",
        "\n",
        "### Subtask:\n",
        "Visualize the cumulative returns of each recommendation type based on the historical picks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae3a81d8"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a line plot of the cumulative returns of each recommendation type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e5c689c"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(cumulative_returns.index, cumulative_returns['Diamond Pick'], label='Diamond Pick')\n",
        "# plt.plot(cumulative_returns.index, cumulative_returns['Golden Pick'], label='Golden Pick')\n",
        "# plt.plot(cumulative_returns.index, cumulative_returns['Silver Pick'], label='Silver Pick')\n",
        "# plt.title('Cumulative Returns of Recommendation Types')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Cumulative Return')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8e4629"
      },
      "source": [
        "## Generate charts and save them as image files\n",
        "\n",
        "### Subtask:\n",
        "Modify the plotting code to save each combined technical indicator chart as a temporary image file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f503d2f5"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the existing plotting code to save each combined technical indicator chart as a temporary image file and store the filenames for later use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b20de3a",
        "outputId": "b91ae1ae-d04f-49e2-c857-adf7e7b9c008"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Convert 'Date' column to datetime objects if not already\n",
        "combined_tick_df['Date'] = pd.to_datetime(combined_tick_df['Date'])\n",
        "\n",
        "# Get the list of unique tickers from the filtered latest two days dataframe\n",
        "# This ensures we only plot for tickers that met the initial filtering criteria\n",
        "unique_shortlisted_tickers = filtered_latest_two_days_df['Ticker'].unique()\n",
        "\n",
        "# List to store temporary plot filenames\n",
        "temp_plot_files = []\n",
        "\n",
        "# Plot for each shortlisted ticker\n",
        "indicators = ['RSI', 'MFI', 'MACD', 'Williams_%R', 'cci', 'OBV', 'AD_Line']\n",
        "num_indicators = len(indicators)\n",
        "\n",
        "for ticker in unique_shortlisted_tickers:\n",
        "    # Find the latest date entry for the current ticker in the filtered DataFrame\n",
        "    latest_date_entry = filtered_latest_two_days_df[filtered_latest_two_days_df['Ticker'] == ticker].iloc[-1]\n",
        "    date_of_signal = latest_date_entry['Date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # Find the corresponding full data for plotting from the combined_tick_df\n",
        "    ticker_plot_df = combined_tick_df[combined_tick_df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # Create a figure with subplots for SMA and Technical Indicators\n",
        "    # Adjust figure size\n",
        "    fig, axes = plt.subplots(nrows=num_indicators + 1, ncols=1, figsize=(12, 2 * (num_indicators + 1)), sharex=True)\n",
        "    fig.suptitle(f'Technical Analysis ({date_of_signal})', y=1.02) # Add a title for the entire figure\n",
        "\n",
        "    # Add ticker name to the top left\n",
        "    fig.text(0.01, 0.99, ticker, # Position in the top left of the figure\n",
        "             horizontalalignment='left', verticalalignment='top', transform=fig.transFigure,\n",
        "             fontsize=12, fontweight='bold')\n",
        "\n",
        "\n",
        "    # Plot SMA on the top subplot\n",
        "    ax_sma = axes[0]\n",
        "    ax_sma.plot(ticker_plot_df['Date'], ticker_plot_df['Close'] , label='Close Price') # Display actual price\n",
        "    ax_sma.plot(ticker_plot_df['Date'], ticker_plot_df['SMA20'] , label='SMA 20')\n",
        "    ax_sma.plot(ticker_plot_df['Date'], ticker_plot_df['SMA50'] , label='SMA 50')\n",
        "    ax_sma.plot(ticker_plot_df['Date'], ticker_plot_df['SMA200'] , label='SMA 200')\n",
        "\n",
        "    ax_sma.set_ylabel('Price') # Changed ylabel to Price\n",
        "    ax_sma.set_title('Stock Price and SMAs')\n",
        "    ax_sma.grid(True)\n",
        "\n",
        "    # Display current values of Close and SMAs within the subplot (aligned right)\n",
        "    current_close = ticker_plot_df['Close'].iloc[-1]\n",
        "    current_sma20 = ticker_plot_df['SMA20'].iloc[-1]\n",
        "    current_sma50 = ticker_plot_df['SMA50'].iloc[-1]\n",
        "    current_sma200 = ticker_plot_df['SMA200'].iloc[-1]\n",
        "    text_to_display_sma = f'Current:\\nClose: {current_close:.2f}\\nSMA20: {current_sma20:.2f}\\nSMA50: {current_sma50:.2f}\\nSMA200: {current_sma200:.2f}'\n",
        "    ax_sma.text(1.01, 0.5, text_to_display_sma, # Position to the right of the plot area\n",
        "             horizontalalignment='left', verticalalignment='center', transform=ax_sma.transAxes,\n",
        "             bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "    ax_sma.legend(loc='upper left', bbox_to_anchor=(0, 1.02)) # Move legend to top left inside plot\n",
        "\n",
        "\n",
        "    # Plot Technical Indicators on the subsequent subplots\n",
        "    for i, indicator in enumerate(indicators):\n",
        "        ax = axes[i + 1] # Start from the second subplot\n",
        "        ax.plot(ticker_plot_df['Date'], ticker_plot_df[indicator], label=indicator)\n",
        "\n",
        "        # Add threshold lines and shaded areas based on the indicator\n",
        "        if indicator == 'MACD':\n",
        "            ax.axhline(0, color='red', linestyle='--')\n",
        "            # Shade area above zero\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], 0, where=(ticker_plot_df[indicator] > 0), color='green', alpha=0.3, interpolate=True)\n",
        "            # Shade area below zero\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], 0, where=(ticker_plot_df[indicator] < 0), color='red', alpha=0.3, interpolate=True)\n",
        "\n",
        "            # Overlay MACD Signal\n",
        "            ax.plot(ticker_plot_df['Date'], ticker_plot_df['MACD_Signal'], label='MACD Signal', linestyle='--')\n",
        "\n",
        "            # Display current values and difference within the subplot (aligned right)\n",
        "            current_macd = ticker_plot_df['MACD'].iloc[-1]\n",
        "            current_signal = ticker_plot_df['MACD_Signal'].iloc[-1]\n",
        "            macd_diff = current_macd - current_signal\n",
        "            text_to_display_corner = f'Current:\\nMACD: {current_macd:.2f}\\nSignal: {current_signal:.2f}\\nDiff: {macd_diff:.2f}'\n",
        "            ax.text(1.01, 0.5, text_to_display_corner, # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "\n",
        "        elif indicator == 'Williams_%R':\n",
        "            threshold_upper = -20\n",
        "            threshold_lower = -80\n",
        "            ax.axhline(threshold_upper, color='red', linestyle='--')\n",
        "            ax.axhline(threshold_lower, color='red', linestyle='--')\n",
        "            # Shade area above -20\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_upper, where=(ticker_plot_df[indicator] > threshold_upper), color='red', alpha=0.3, interpolate=True)\n",
        "            # Shade area below -80\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_lower, where=(ticker_plot_df[indicator] < threshold_lower), color='green', alpha=0.3, interpolate=True)\n",
        "\n",
        "            # Display current value within the subplot (aligned right)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "            # Add threshold labels within the subplot (aligned right), vertically aligned with lines\n",
        "            ax.text(1.01, threshold_upper, f'Upper: {threshold_upper}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "            ax.text(1.01, threshold_lower, f'Lower: {threshold_lower}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "\n",
        "\n",
        "        elif indicator == 'RSI':\n",
        "            threshold_upper = 70\n",
        "            threshold_lower = 30\n",
        "            ax.axhline(threshold_upper, color='red', linestyle='--')\n",
        "            ax.axhline(threshold_lower, color='red', linestyle='--')\n",
        "            # Shade area above 70\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_upper, where=(ticker_plot_df[indicator] > threshold_upper), color='red', alpha=0.3, interpolate=True)\n",
        "            # Shade area below 30\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_lower, where=(ticker_plot_df[indicator] < threshold_lower), color='green', alpha=0.3, interpolate=True)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "            ax.text(1.01, threshold_upper, f'Upper: {threshold_upper}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "            ax.text(1.01, threshold_lower, f'Lower: {threshold_lower}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "\n",
        "\n",
        "        elif indicator == 'MFI':\n",
        "            threshold_upper = 80\n",
        "            threshold_lower = 20\n",
        "            ax.axhline(threshold_upper, color='red', linestyle='--')\n",
        "            ax.axhline(threshold_lower, color='red', linestyle='--')\n",
        "            # Shade area above 80\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_upper, where=(ticker_plot_df[indicator] > threshold_upper), color='red', alpha=0.3, interpolate=True)\n",
        "            # Shade area below 20\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_lower, where=(ticker_plot_df[indicator] < threshold_lower), color='green', alpha=0.3, interpolate=True)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "            ax.text(1.01, threshold_upper, f'Upper: {threshold_upper}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "            ax.text(1.01, threshold_lower, f'Lower: {threshold_lower}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "\n",
        "\n",
        "        elif indicator == 'cci':\n",
        "            threshold_upper = 200\n",
        "            threshold_lower = -200\n",
        "            ax.axhline(threshold_upper, color='red', linestyle='--')\n",
        "            ax.axhline(threshold_lower, color='red', linestyle='--')\n",
        "            # Shade area above 200\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_upper, where=(ticker_plot_df[indicator] > threshold_upper), color='red', alpha=0.3, interpolate=True)\n",
        "            # Shade area below -200\n",
        "            ax.fill_between(ticker_plot_df['Date'], ticker_plot_df[indicator], threshold_lower, where=(ticker_plot_df[indicator] < threshold_lower), color='green', alpha=0.3, interpolate=True)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "            ax.text(1.01, threshold_upper, f'Upper: {threshold_upper}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "            ax.text(1.01, threshold_lower, f'Lower: {threshold_lower}', # Position to the right of the plot area, aligned with line\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.get_yaxis_transform(), fontsize=9) # Use yaxis transform for data alignment\n",
        "\n",
        "\n",
        "        elif indicator == 'OBV':\n",
        "             # Display current value within the subplot (aligned right)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "\n",
        "        elif indicator == 'AD_Line':\n",
        "             # Display current value within the subplot (aligned right)\n",
        "            current_value = ticker_plot_df[indicator].iloc[-1]\n",
        "            ax.text(1.01, 0.5, f'Current {indicator}: {current_value:.2f}', # Position to the right of the plot area\n",
        "                     horizontalalignment='left', verticalalignment='center', transform=ax.transAxes,\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "\n",
        "        ax.set_ylabel(indicator)\n",
        "        ax.legend(loc='upper left', bbox_to_anchor=(0, 1.02)) # Move legend to top left inside plot\n",
        "\n",
        "\n",
        "        # Format x-axis to show months for the bottom subplot\n",
        "        if i == num_indicators - 1:\n",
        "            formatter = mdates.DateFormatter('%Y-%m')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "            plt.xticks(rotation=45)\n",
        "        else:\n",
        "            ax.tick_params(labelbottom=False) # Hide x-axis labels for upper subplots\n",
        "\n",
        "        ax.grid(True) # Keep grid\n",
        "\n",
        "\n",
        "    plt.tight_layout() # Use default tight_layout\n",
        "\n",
        "\n",
        "    # Save the combined plot to a temporary file\n",
        "    combined_plot_filename = f'{ticker}_technical_analysis_plot.png'\n",
        "    plt.savefig(combined_plot_filename)\n",
        "    plt.close(fig) # Close the figure to free up memory\n",
        "    temp_plot_files.append(combined_plot_filename) # Add to list for cleanup\n",
        "\n",
        "print(f\"Generated and saved {len(temp_plot_files)} temporary plot files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c767895"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new folder in Google Drive with the current date as the folder name to store the technical analysis plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3dea7b7",
        "outputId": "38b6b5d3-dc25-46df-c79e-0a938d2ebaad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "# This is already done in a previous cell, so we just assume it's mounted at /content/drive\n",
        "\n",
        "# Define the base target directory in Google Drive\n",
        "drive_base_path = '/content/drive/MyDrive/stock_analysis_plots' # Adjust this path as needed\n",
        "\n",
        "# Create a date-stamped folder name\n",
        "date_today = datetime.now().strftime('%Y-%m-%d')\n",
        "drive_date_folder = os.path.join(drive_base_path, date_today)\n",
        "\n",
        "# Create the date directory if it doesn't exist\n",
        "os.makedirs(drive_date_folder, exist_ok=True)\n",
        "\n",
        "# Define and create subfolders for each recommendation type within the date folder\n",
        "recommendation_types = ['Diamond Pick', 'Golden Pick', 'Silver Pick']\n",
        "drive_subfolders = {}\n",
        "for rec_type in recommendation_types:\n",
        "    # Create a folder name that is filesystem-friendly (replace spaces with underscores)\n",
        "    folder_name = rec_type.replace(' ', '_')\n",
        "    subfolder_path = os.path.join(drive_date_folder, folder_name)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "    drive_subfolders[rec_type] = subfolder_path # Store the path mapped to the original recommendation type\n",
        "\n",
        "print(f\"Google Drive target date folder created: {drive_date_folder}\")\n",
        "print(\"Google Drive subfolders for recommendations created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67524931"
      },
      "source": [
        "**Reasoning**:\n",
        "Copy the temporary plot files to the newly created Google Drive folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed23e70d",
        "outputId": "69341a1f-62a0-450e-eac3-c97a8f3030eb"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import pandas as pd # Import pandas to access filtered_latest_two_days_df\n",
        "\n",
        "# Assuming filtered_latest_two_days_df is available from a previous cell and contains 'Ticker' and 'Recommendation'\n",
        "\n",
        "# Copy each temporary plot file to the appropriate Google Drive subfolder\n",
        "for plot_file in temp_plot_files:\n",
        "    # Extract ticker name from the filename (assuming filename is in the format \"TICKER_...\")\n",
        "    ticker = plot_file.split('_')[0]\n",
        "\n",
        "    # Find the recommendation for this ticker from the filtered_latest_two_days_df\n",
        "    # We need to find the latest recommendation for the ticker\n",
        "    ticker_latest_recommendation_row = filtered_latest_two_days_df[filtered_latest_two_days_df['Ticker'] == ticker].tail(1)\n",
        "\n",
        "    if not ticker_latest_recommendation_row.empty:\n",
        "        recommendation = ticker_latest_recommendation_row['Recommendation'].iloc[0]\n",
        "\n",
        "        # Get the target subfolder path based on the recommendation\n",
        "        # Use the dictionary created in the previous cell\n",
        "        if recommendation in drive_subfolders:\n",
        "            target_subfolder = drive_subfolders[recommendation]\n",
        "            shutil.copy(plot_file, target_subfolder)\n",
        "            print(f\"Copied {plot_file} to {target_subfolder}\")\n",
        "        else:\n",
        "            print(f\"Warning: Could not find a matching subfolder for recommendation '{recommendation}' for ticker {ticker}. Skipping copy for this plot.\")\n",
        "    else:\n",
        "        print(f\"Warning: Could not find recommendation data for ticker {ticker} in filtered_latest_two_days_df. Skipping copy for this plot.\")\n",
        "\n",
        "\n",
        "print(\"All temporary plot files copied to Google Drive subfolders.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14eb2154"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate a shareable link for the newly created Google Drive folder containing the plots and include this link in the email report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7e34899"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `receiver_email` was not defined. Define the `receiver_email` variable and retry sending the email with the shortlisted tickers report, performance metrics, new signal analysis, and the Google Drive folder reference, including the cleanup of temporary plot files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eYmlN7KA7Pn",
        "outputId": "8ebe6148-fdd7-4ffa-ffd1-69fcc69a1063"
      },
      "outputs": [],
      "source": [
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Email details\n",
        "sender_email = gmail_address\n",
        "sender_password = gmail_app_password\n",
        "receiver_email = gmail_address # Define the receiver email here\n",
        "subject = \"Performance Reports for Shortlisted Tickers\" # Updated subject\n",
        "\n",
        "# Prepare the email body with the shortlisted tickers data and performance metrics\n",
        "email_body = \"\" # Initialize email body\n",
        "\n",
        "if not filtered_latest_two_days_df.empty:\n",
        "    shortlisted_tickers_list = filtered_latest_two_days_df['Ticker'].unique().tolist()\n",
        "\n",
        "    # Add the shortlisted tickers data (Diamond, Golden, Silver Picks)\n",
        "    email_body += \"<h2>Shortlisted Stock Tickers (Diamond, Golden, and Silver Picks)</h2>\\n\"\n",
        "    email_body += \"<p>Here are the stock tickers that met the filtering criteria for the latest trading sessions:</p>\\n\"\n",
        "    email_body += filtered_latest_two_days_df[['Date', 'Ticker', 'Recommendation', 'satisfied_conditions_count', 'Satisfied_Conditions_Description']].to_html(index=False)\n",
        "    email_body += \"\\n<br>\\n\" # Add a break\n",
        "\n",
        "\n",
        "    # Add the latest new signals triggered by shortlisted tickers\n",
        "    if 'latest_new_signals_for_shortlisted' in locals() and latest_new_signals_for_shortlisted:\n",
        "        email_body += \"<h2>Latest New Signals Triggered by Shortlisted Tickers</h2>\\n\"\n",
        "        email_body += \"<p>For the latest trading session, the following new signals were triggered by the shortlisted tickers:</p>\\n\"\n",
        "        email_body += \"<ul>\\n\"\n",
        "        for ticker, signals in latest_new_signals_for_shortlisted.items():\n",
        "            email_body += f\"<li><b>{ticker}:</b> {signals}</li>\\n\"\n",
        "        email_body += \"</ul>\\n\"\n",
        "        email_body += \"\\n<br>\\n\" # Add a break\n",
        "\n",
        "\n",
        "    if 'performance_metrics_by_ticker_recommendation_df' in locals() and not performance_metrics_by_ticker_recommendation_df.empty:\n",
        "        # Filter the performance metrics DataFrame for original picks for only the shortlisted tickers\n",
        "        performance_metrics_shortlisted_original = performance_metrics_by_ticker_recommendation_df.loc[performance_metrics_by_ticker_recommendation_df.index.get_level_values('Ticker').isin(shortlisted_tickers_list)].copy()\n",
        "\n",
        "        # Add the original performance metrics as an HTML table\n",
        "        email_body += \"<h2>Historical Performance Metrics for Original Picks (Grouped by Ticker and Recommendation)</h2>\\n\"\n",
        "        email_body += performance_metrics_shortlisted_original.to_html() # Add the performance metrics as an HTML table\n",
        "        # Add description for original performance metrics columns\n",
        "        email_body += \"<p><b>Column Descriptions:</b></p>\\n\"\n",
        "        email_body += \"<ul>\\n\"\n",
        "        email_body += \"<li><b>Total Picks:</b> The total number of times this Ticker and Recommendation combination occurred in the backtesting period.</li>\\n\"\n",
        "        email_body += \"<li><b>Winning Picks:</b> The number of times the price reached a new high within the holding period after the pick date (indicating a positive return).</li>\\n\"\n",
        "        email_body += \"<li><b>Losing Picks:</b> The number of times the price did not reach a new high within the holding period (indicating a zero or negative return).</li>\\n\"\n",
        "        email_body += \"<li><b>Win Rate (%):</b> The percentage of Winning Picks out of Total Picks.</li>\\n\"\n",
        "        email_body += \"<li><b>Loss Rate (%):</b> The percentage of Losing Picks out of Total Picks.</li>\\n\"\n",
        "        email_body += \"<li><b>Average Winning Return (%):</b> The average percentage return for all Winning Picks in this category.</li>\\n\"\n",
        "        email_body += \"<li><b>Average Losing Return (%):</b> The average percentage return for all Losing Picks in this category.</li>\\n\"\n",
        "        email_body += \"<li><b>Average Total Return (%):</b> The average percentage return for all Picks (Winning and Losing) in this category.</li>\\n\"\n",
        "        email_body += \"</ul>\\n\"\n",
        "        email_body += \"\\n<br>\\n\" # Add a break between sections\n",
        "\n",
        "\n",
        "    if 'signal_performance_by_ticker_df' in locals() and not signal_performance_by_ticker_df.empty:\n",
        "         # Filter the performance metrics for new signals for only the shortlisted tickers\n",
        "         # The index of signal_performance_by_ticker_df is MultiIndex (Signal, Ticker)\n",
        "         # We need to filter based on the Ticker level of the index\n",
        "         signal_performance_shortlisted_new = signal_performance_by_ticker_df.loc[signal_performance_by_ticker_df.index.get_level_values('Ticker').isin(shortlisted_tickers_list)].copy()\n",
        "\n",
        "\n",
        "         # Add the new signal performance metrics as an HTML table\n",
        "         email_body += \"<h2>Winning Percentage for New Signal Conditions (Grouped by Signal and Ticker)</h2>\\n\"\n",
        "         email_body += signal_performance_shortlisted_new.to_html() # Add the new signal performance metrics as an HTML table\n",
        "         # Add description for new signal performance metrics columns\n",
        "         email_body += \"<p><b>Column Descriptions:</b></p>\\n\"\n",
        "         email_body += \"<ul>\\n\"\n",
        "         email_body += \"<li><b>Total Triggers:</b> The total number of times this Signal and Ticker combination was triggered in the backtesting period.</li>\\n\"\n",
        "         email_body += \"<li><b>Winning Triggers (based on Max Price):</b> The number of times the price reached a new high within the holding period after the signal trigger date (indicating a positive return).</li>\\n\"\n",
        "         email_body += \"<li><b>Winning Percentage (%):</b> The percentage of Winning Triggers out of Total Triggers for this Signal and Ticker.</li>\\n\"\n",
        "         email_body += \"</ul>\\n\"\n",
        "         email_body += \"\\n<br>\\n\" # Add a break\n",
        "\n",
        "    # Define the target directory in Google Drive (replicated from the previous successful cell)\n",
        "    date_today = datetime.now().strftime('%Y-%m-%d')\n",
        "    drive_base_path = '/content/drive/MyDrive/stock_analysis_plots' # Adjust this path as needed\n",
        "    drive_target_folder = os.path.join(drive_base_path, date_today)\n",
        "\n",
        "    # Add the instruction/link to the Google Drive folder\n",
        "    drive_link_instruction = f\"Please find the technical analysis plots in the Google Drive folder '{date_today}' within '{drive_base_path}'. You can navigate to this folder in your Google Drive and generate a shareable link if needed.\"\n",
        "    email_body += f\"<p>{drive_link_instruction}</p>\\n\"\n",
        "\n",
        "    # Add technical indicator significance (reusing from 4c0258a2)\n",
        "    email_body += \"<h2>Technical Indicator Significance</h2>\\n\"\n",
        "    email_body += \"<p>Here is a brief explanation of the technical indicators used in the analysis and plots:</p>\\n\"\n",
        "    email_body += \"<ul>\\n\"\n",
        "    email_body += \"<li><b>RSI (Relative Strength Index):</b> A momentum oscillator that measures the speed and change of price movements. RSI values range from 0 to 100. Generally, RSI above 70 indicates overbought conditions, and below 30 indicates oversold conditions.</li>\\n\"\n",
        "    email_body += \"<li><b>MFI (Money Flow Index):</b> A momentum indicator that uses both price and volume to measure buying and selling pressure. MFI values range from 0 to 100. Generally, MFI above 80 indicates overbought conditions, and below 20 indicates oversold conditions.</li>\\n\"\n",
        "    email_body += \"<li><b>MACD (Moving Average Convergence Divergence):</b> A trend-following momentum indicator that shows the relationship between two moving averages of a security’s price. The MACD line crossing above the Signal Line is typically a bullish signal, and crossing below is a bearish signal.</li>\\n\"\n",
        "    email_body += \"<li><b>Williams %R:</b> A momentum indicator that measures overbought and oversold levels. Williams %R values range from 0 to -100. Generally, a reading between 0 and -20 is considered overbought, and between -80 and -100 is considered oversold.</li>\\n\"\n",
        "    email_body += \"<li><b>CCI (Commodity Channel Index):</b> An oscillator used to identify cyclical trends. CCI measures the difference between a security's price change and its average price change. Generally, readings above +100 suggest overbought conditions, and below -100 suggest oversold conditions. Some traders use wider thresholds like +200 and -200 for stronger signals.</li>\\n\"\n",
        "    email_body += \"<li><b>OBV (On-Balance Volume):</b> A momentum indicator that relates volume to price change. OBV is a running total of volume, where volume is added on up days and subtracted on down days. A rising OBV confirms a rising price trend, while a falling OBV confirms a falling price trend. Divergences between price and OBV can signal potential reversals.</li>\\n\"\n",
        "    email_body += \"<li><b>A/D Line (Accumulation/Distribution Line):</b> A volume-based indicator that assesses buying and selling pressure by measuring the cumulative flow of money into and out of a security. A rising A/D line suggests accumulation (buying pressure), while a falling A/D line suggests distribution (selling pressure). It's often used to confirm trends or spot divergences.</li>\\n\"\n",
        "    email_body += \"</ul>\\n\"\n",
        "    email_body += \"\\n<br>\\n\"\n",
        "\n",
        "\n",
        "else:\n",
        "     email_body = \"Dear User,\\n\\nNo relevant backtesting performance metrics found for the shortlisted tickers.\"\n",
        "\n",
        "\n",
        "# Create the MIMEMultipart message\n",
        "msg = MIMEMultipart() # Re-initialize msg\n",
        "msg['From'] = sender_email\n",
        "msg['To'] = receiver_email\n",
        "msg['Subject'] = subject\n",
        "msg.attach(MIMEText(email_body, 'html'))\n",
        "\n",
        "# Note: The plot files were generated and saved to Google Drive in previous steps.\n",
        "# We are not attaching them here, only providing the link in the email body.\n",
        "\n",
        "# Connect to the Gmail SMTP server and send the email\n",
        "try:\n",
        "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "        server.login(sender_email, sender_password)\n",
        "        server.sendmail(sender_email, receiver_email, msg.as_string())\n",
        "    print(\"Email with performance reports and Google Drive link sent successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error sending email: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up temporary plot files (assuming they were created in a previous step and need cleanup)\n",
        "    # This assumes a variable 'temp_plot_files' exists and contains the list of temporary file paths\n",
        "    if 'temp_plot_files' in locals():\n",
        "        for plot_file in temp_plot_files:\n",
        "            if os.path.exists(plot_file):\n",
        "                os.remove(plot_file)\n",
        "        if temp_plot_files:\n",
        "            print(\"Temporary plot files removed.\")\n",
        "    else:\n",
        "        print(\"No temporary plot files to remove.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2d5774"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The process successfully generated and saved 19 temporary technical analysis plot files for the shortlisted tickers.\n",
        "*   A new folder named with the current date was created in the specified Google Drive location.\n",
        "*   All generated temporary plot files were successfully copied to the date-stamped Google Drive folder.\n",
        "*   The email report was successfully sent, containing the shortlisted tickers data, historical performance metrics for original picks and new signal conditions, and an instruction to access the technical analysis plots in the Google Drive folder.\n",
        "*   The temporary plot files were successfully cleaned up after the email was sent.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current process provides instructions on how to find the plots in Google Drive but does not include a direct shareable link. A next step could be to programmatically generate a shareable link for the uploaded folder and include that link directly in the email for easier access.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
